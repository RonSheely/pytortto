{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../../src')\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tortto as tt\n",
    "import tortto.nn as nn\n",
    "import tortto.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './cifar10_data'\n",
    "img_mean = (0.485, 0.456, 0.406)\n",
    "img_sd = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((224,224)),\n",
    "    ToTensor(),\n",
    "    Normalize(img_mean, img_sd),\n",
    "])\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, downsample=None):\n",
    "        \"\"\"\n",
    "        in_channels: number of incoming channels.\n",
    "        channels: number of channels in the first layer of this block\n",
    "        \"\"\"\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(in_channels, channels)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = conv3x3(channels, channels, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = conv1x1(channels, channels * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(channels * self.expansion)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x---conv1,bn1,relu-->conv2,bn2,relu-->conv3,bn3---relu-->\n",
    "          |_____________ downsample_____________________|\n",
    "        \"\"\"\n",
    "        shortcut = x if self.downsample is None else self.downsample(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        return F.relu(out + shortcut)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, channels):\n",
    "        \"\"\"\n",
    "        layers: number of residual blocks in each layer\n",
    "        channels: number of channels\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = channels[0]\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)  # in_channels will increase accordingly after each _make_layer call\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, channels[0], layers[0])\n",
    "        self.layer2 = self._make_layer(block, channels[1], layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, channels, blocks, stride=1):\n",
    "        downsample = nn.Sequential()\n",
    "        # for the first block of conv2_x, if bottleneck is used, there will be increase of channels\n",
    "        # so we need downsample\n",
    "        if stride != 1 or self.in_channels != channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_channels, channels * block.expansion, stride),\n",
    "                nn.BatchNorm2d(channels * block.expansion)\n",
    "            )\n",
    "\n",
    "        # append the first block in the layer\n",
    "        layers = [block(self.in_channels, channels, stride, downsample)]\n",
    "        self.in_channels = channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, channels)  # stride=1\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        feature_maps = x.data.copy()\n",
    "        x = tt.mean(x, (-1, -2), keepdims=True)\n",
    "        x = tt.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        self.cam = (self.classifier.weight.data[x.data.argmax()][None,...,None,None] * feature_maps).sum(-3)[0]\n",
    "        return F.log_softmax(x, -1)\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jacobgil/vit-explain/blob/main/vit_explain.py\n",
    "# he made a tiny mistake: heatmap from cv2 is in bgr format, so need to convert to rgb before adding to the img.\n",
    "def show_mask_on_image(img, mask):\n",
    "    img = np.float32(img) / 255\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = resnet50()\n",
    "net.classifier = nn.Linear(2048,10)\n",
    "net = net.cuda()\n",
    "\n",
    "checkpoint = tt.load('checkpoint_014.npy')\n",
    "net.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uncomment to plot class activation map (CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fns=os.listdir('images')\n",
    "# fig,axs=plt.subplots(len(fns),4,figsize=(12,43))\n",
    "\n",
    "# axs[0,0].set_title('Original',fontsize=15)\n",
    "# axs[0,1].set_title('Class Activation Map',fontsize=15)\n",
    "# axs[0,2].set_title('Overlap',fontsize=15)\n",
    "# axs[0,3].set_title('Prediction',fontsize=15)\n",
    "# i=0\n",
    "# for fn in fns:\n",
    "#     fn='images/'+fn\n",
    "#     original=Image.open(fn).convert('RGB')\n",
    "#     data=tt.tensor(transform(original)[None].numpy()).cuda()\n",
    "\n",
    "#     net.eval()\n",
    "#     with tt.no_grad():\n",
    "#         outputs=net(data)\n",
    "#         predicted=classes[outputs.argmax(-1).item()]\n",
    "#     prob=outputs.exp().detach().cpu().numpy()\n",
    "#     mask=net.cam.get()\n",
    "#     mask=cv2.resize(mask, (224, 224))\n",
    "#     mask-=mask.min()\n",
    "#     mask/=mask.max()\n",
    "#     data=data.cpu().numpy()[0]\n",
    "#     for t, m, s in zip(data, img_mean, img_sd):\n",
    "#         t*=s\n",
    "#         t+=m\n",
    "#     img=show_mask_on_image(255*(data.transpose(1,2,0)), mask)\n",
    "\n",
    "#     axs[i,0].imshow(original)\n",
    "#     axs[i,0].axis('off')\n",
    "    \n",
    "#     axs[i,1].imshow(mask, cmap='jet')\n",
    "#     axs[i,1].axis('off')\n",
    "\n",
    "#     axs[i,2].imshow(img, cmap='jet')\n",
    "#     axs[i,2].axis('off')\n",
    "\n",
    "#     axs[i,3].bar(classes, prob[0])\n",
    "#     axs[i,3].tick_params('x', labelrotation=45)\n",
    "#     axs[i,3].set_ylim([0, 1.1])\n",
    "#     axs[i,3].set_aspect(10)\n",
    "#     i+=1\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
